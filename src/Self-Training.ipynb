{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a639c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import constant as config\n",
    "import torch\n",
    "from util.dataset import read_dataset, sampling_dataset, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from trainer import Trainer\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "# from util.augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f938566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1333cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 12 10:30:29 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 49%   41C    P8    38W / 370W |    318MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1846      G   /usr/lib/xorg/Xorg                 88MiB |\r\n",
      "|    0   N/A  N/A      2327      G   /usr/bin/gnome-shell               72MiB |\r\n",
      "|    0   N/A  N/A      2774      G   ...mviewer/tv_bin/TeamViewer        4MiB |\r\n",
      "|    0   N/A  N/A    213346      G   /usr/lib/firefox/firefox          147MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4575e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = list(zip(['one', 'two', 'three'], [1, 2 , 3]))\n",
    "print(dataset.pop(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9450f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled num 7927, unlabeled num 10000, valid num 986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "set_seed(42)\n",
    "fold = 10\n",
    "# percentage = \"10_percent\"\n",
    "# train_path = f'../spin-off/dataset/fold_{fold}/10-fold_original_{percentage}/algocite_utilize_dataset_train_fold_{fold}.csv'\n",
    "# test_path = f'../spin-off/dataset/fold_{fold}/10-fold_original_{percentage}/algocite_utilize_dataset_test_fold_{fold}.csv'\n",
    "# val_path = f'../spin-off/dataset/fold_{fold}/10-fold_original_{percentage}/algocite_utilize_dataset_val_fold_{fold}.csv'\n",
    "train_path = f'../dataset/10-fold_labeled_increasing_relabel_ver3/algocite_utilize_dataset_train_fold_{fold}.csv'\n",
    "test_path = f'../dataset/10-fold_labeled_increasing_relabel_ver3/algocite_utilize_dataset_test_fold_{fold}.csv'\n",
    "val_path = f'../dataset/10-fold_labeled_increasing_relabel_ver3/algocite_utilize_dataset_val_fold_{fold}.csv'\n",
    "unlabel_path = f'../spin-off/algocitecontexts_unlabeled_10000_random_new1.csv'\n",
    "\n",
    "# Read dataset\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_unlabeled = pd.read_csv(unlabel_path)\n",
    "\n",
    "# df_unlabeled.dropna(inplace=True)\n",
    "\n",
    "train_texts, train_labels = df_train['CITATIONS_CONTEXTS'].values, df_train['USAGE_LABELS'].values\n",
    "test_texts, test_labels = df_test['CITATIONS_CONTEXTS'].values, df_test['USAGE_LABELS'].values\n",
    "val_texts, val_labels = df_val['CITATIONS_CONTEXTS'].values, df_val['USAGE_LABELS'].values\n",
    "unlabeled_texts, unlabeled_labels = df_unlabeled['CONTENTS'].values, df_unlabeled['LABELS'].values\n",
    "\n",
    "labeled_data = list(zip(train_texts, train_labels))\n",
    "unlabeled_data = list(zip(unlabeled_texts, unlabeled_labels))\n",
    "dev_data = list(zip(val_texts, val_labels))\n",
    "\n",
    "print('labeled num {}, unlabeled num {}, valid num {}'.format(len(labeled_data), len(unlabeled_data), len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60675479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing \n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=True)\n",
    "\n",
    "# encoding = { \n",
    "#     'NOTUTILIZE': 0,\n",
    "#     'UTILIZE': 1,\n",
    "# }\n",
    "\n",
    "encoding = { \n",
    "    'EXTEND': 0,\n",
    "    'MENTION': 1,\n",
    "    'NOTALGO': 2,\n",
    "    'USE': 3\n",
    "}\n",
    "\n",
    "unlabeled_encoding = {\n",
    "    'UNK_UNK': 0\n",
    "}\n",
    "\n",
    "labeled_texts = [data[0] for data in labeled_data]\n",
    "labeled_labels = [data[1] for data in labeled_data]\n",
    "labeled_labels = [encoding[key] for key in labeled_labels]\n",
    "labeled_encodings = tokenizer(labeled_texts, \n",
    "                              add_special_tokens=True, \n",
    "                              max_length=256,\n",
    "                              truncation=True, \n",
    "                              padding='max_length',\n",
    "                              return_attention_mask=True\n",
    "                             )\n",
    "labeled_dataset = Dataset(labeled_encodings, labeled_labels)\n",
    "\n",
    "dev_texts = [data[0] for data in dev_data]\n",
    "dev_labels = [data[1] for data in dev_data]\n",
    "dev_labels = [encoding[key] for key in dev_labels]\n",
    "dev_encodings = tokenizer(dev_texts, \n",
    "                          add_special_tokens=True, \n",
    "                          max_length=256,\n",
    "                          truncation=True, \n",
    "                          padding='max_length',\n",
    "                          return_attention_mask=True\n",
    "                         )\n",
    "dev_dataset = Dataset(dev_encodings, dev_labels)\n",
    "\n",
    "test_texts = list(test_texts)\n",
    "test_labels = [encoding[key] for key in test_labels]\n",
    "test_encodings = tokenizer(test_texts, \n",
    "                           add_special_tokens=True, \n",
    "                           max_length=256,\n",
    "                           truncation=True, \n",
    "                           padding='max_length',\n",
    "                           return_attention_mask=True\n",
    "                          )\n",
    "test_dataset = Dataset(test_encodings, test_labels)\n",
    "\n",
    "# We keep the label of unlabeled data to track for accuracy of pseudo-labeling\n",
    "unlabeled_texts = [data[0] for data in unlabeled_data]\n",
    "unlabeled_labels = [data[1] for data in unlabeled_data]\n",
    "unlabeled_labels = [unlabeled_encoding[key] for key in unlabeled_labels]\n",
    "unlabeled_encodings = tokenizer(unlabeled_texts, \n",
    "                                add_special_tokens=True, \n",
    "                                max_length=256,\n",
    "                                truncation=True, \n",
    "                                padding='max_length',\n",
    "                                return_attention_mask=True\n",
    "                               )\n",
    "unlabeled_dataset = Dataset(unlabeled_encodings, unlabeled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e011646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 1, 1, 1, 1, 3, 1, 0, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 2, 3, 3, 3, 2, 3, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 2, 1, 3, 1, 1, 1, 1, 3, 3, 2, 3, 1, 2, 3, 2, 3, 1, 0, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 0, 1, 3, 3, 2, 1, 1, 2, 3, 3, 2, 1, 3, 1, 1, 3, 1, 1, 1, 1, 2, 0, 3, 3, 3, 1, 3, 1, 3, 3, 2, 1, 3, 1, 1, 1, 1, 1, 0, 3, 1, 3, 1, 3, 2, 3, 2, 3, 2, 1, 1, 3, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 3, 1, 1, 1, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 3, 2, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 2, 3, 1, 2, 0, 3, 3, 1, 1, 2, 3, 1, 1, 3, 3, 1, 3, 1, 1, 1, 0, 3, 3, 3, 1, 1, 1, 3, 1, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 1, 0, 1, 3, 1, 1, 0, 3, 3, 1, 2, 2, 1, 1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 2, 1, 1, 2, 3, 1, 3, 3, 3, 2, 1, 3, 0, 2, 1, 3, 2, 1, 3, 1, 3, 3, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 3, 3, 0, 1, 1, 3, 1, 3, 1, 3, 1, 3, 3, 1, 3, 1, 2, 1, 1, 1, 1, 3, 3, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 0, 3, 3, 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 3, 3, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 3, 2, 1, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 3, 1, 3, 1, 3, 1, 0, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 2, 1, 1, 3, 1, 2, 1, 3, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 1, 1, 2, 1, 3, 1, 3, 3, 1, 1, 3, 3, 3, 2, 1, 1, 3, 2, 2, 1, 3, 1, 3, 3, 1, 1, 1, 1, 2, 3, 3, 1, 3, 1, 3, 3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 3, 2, 3, 1, 3, 1, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 3, 1, 1, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 2, 3, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 3, 2, 3, 3, 1, 3, 1, 3, 1, 3, 3, 2, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 2, 2, 1, 3, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 3, 1, 3, 1, 2, 3, 2, 2, 1, 1, 1, 1, 3, 0, 1, 1, 3, 3, 3, 3, 0, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 3, 1, 3, 3, 1, 3, 0, 1, 1, 1, 0, 3, 1, 1, 1, 3, 1, 3, 1, 2, 1, 1, 3, 1, 3, 3, 3, 3, 2, 1, 2, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 2, 3, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 3, 0, 1, 1, 1, 3, 1, 2, 3, 1, 1, 3, 2, 2, 1, 1, 1, 3, 3, 3, 2, 3, 3, 2, 3, 1, 3, 1, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 3, 1, 1, 2, 1, 1, 1, 0, 3, 1, 1, 1, 3, 1, 2, 3, 1, 0, 1, 3, 3, 2, 1, 3, 1, 1, 1, 1, 1, 2, 3, 1, 2, 1, 2, 1, 1, 2, 3, 1, 1, 1, 1, 3, 1, 2, 3, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 3, 3, 1, 1, 2, 2, 0, 2, 1, 1, 1, 1, 0, 2, 3, 1, 1, 1, 2, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 2, 3, 1, 1, 1, 1, 3, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 0, 3, 1, 1, 2, 2, 0, 3, 1, 1, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 0, 2, 1, 3, 0, 1, 1, 1, 1, 1, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 2, 0, 3, 1, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 3, 3, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 2, 1, 0, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 3, 1, 0, 1, 1, 1, 1, 1, 1, 3, 3, 1, 2, 1, 0, 3, 3, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 3, 3, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 2, 2, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 3, 1, 1, 1, 2, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 3, 1, 0, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 2, 3, 2, 3, 3, 1, 1, 3, 3, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 3, 1, 3, 2, 1, 2, 1, 1, 3, 2, 2, 2, 3, 3, 2, 1, 2, 1, 1, 1, 2, 2, 3, 3, 2, 2, 1, 2, 3, 3, 2, 2, 1, 2, 1, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 1, 3, 3, 1, 2, 1, 3, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 1, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 1, 3, 3, 3, 3, 2, 3, 2, 2, 2, 1, 1, 3, 1, 2, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 0, 0, 1, 0, 3, 3, 2, 1, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 2, 1, 1, 3, 3, 2, 2, 3, 2, 3, 3, 2, 1, 1, 2, 3, 2, 3, 0, 2, 2, 1, 2, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 1, 1, 2, 3, 3, 3, 1, 2, 1, 2, 3, 2, 2, 3, 3, 2, 1, 3, 2, 1, 2, 2, 2, 1, 3, 3, 3, 3, 3, 0, 2, 3, 3, 1, 3, 2, 2, 2, 2, 3, 1, 2, 2, 0, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 2, 1, 2, 2, 2, 3, 3, 3, 1, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 1, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 3, 2, 1, 3, 3, 2, 1, 1, 3, 2, 1, 3, 1, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 2, 3, 3, 2, 3, 3, 1, 3, 2, 2, 3, 3, 1, 3, 3, 2, 2, 2, 3, 3, 3, 1, 3, 3, 3, 3, 1, 1, 3, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 3, 3, 1, 2, 0, 2, 2, 2, 3, 2, 3, 2, 2, 1, 1, 2, 3, 2, 2, 2, 2, 3, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 1, 2, 3, 1, 1, 3, 3, 3, 3, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 3, 3, 3, 3, 2, 1, 3, 3, 0, 1, 2, 2, 3, 3, 3, 2, 2, 2, 1, 3, 1, 2, 0, 2, 3, 2, 2, 1, 3, 2, 2, 1, 3, 1, 1, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3, 1, 2, 3, 1, 1, 3, 3, 3, 3, 2, 1, 2, 2, 3, 3, 2, 2, 3, 1, 2, 2, 3, 2, 2, 2, 2, 2, 1, 2, 3, 1, 1, 0, 3, 3, 3, 2, 1, 1, 1, 3, 1, 1, 2, 3, 3, 3, 1, 1, 3, 2, 3, 3, 3, 3, 3, 1, 3, 2, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2, 2, 1, 3, 1, 1, 3, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 3, 1, 3, 1, 3, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 2, 3, 3, 3, 2, 1, 1, 1, 1, 1, 2, 3, 3, 1, 2, 1, 3, 3, 1, 3, 1, 1, 1, 2, 2, 3, 2, 1, 1, 3, 2, 1, 1, 2, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 2, 2, 3, 0, 1, 1, 1, 2, 3, 0, 3, 1, 3, 3, 2, 1, 3, 1, 2, 1, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 2, 1, 3, 1, 1, 3, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 2, 3, 3, 1, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 2, 2, 3, 3, 3, 3, 2, 1, 3, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 2, 3, 1, 3, 1, 3, 1, 1, 3, 3, 1, 0, 3, 2, 2, 3, 3, 3, 0, 3, 3, 3, 1, 1, 2, 2, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 0, 2, 0, 3, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 2, 1, 0, 2, 1, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 3, 1, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 0, 1, 2, 3, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 0, 2, 2, 2, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 1, 0, 3, 2, 0, 2, 1, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 2, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 1, 1, 0, 1, 0, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 0, 1, 0, 0, 1, 1, 2, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 1, 1, 0, 1, 0, 0, 1, 3, 3, 0, 1, 1, 2, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 1, 1, 3, 0, 1, 1, 0, 1, 2, 3, 1, 2, 0, 1, 2, 2, 1, 2, 2, 2, 0, 1, 1, 2, 2, 2, 3, 0, 2, 2, 0, 1, 1, 2, 2, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 3, 0, 0, 2, 0, 2, 2, 1, 1, 1, 2, 0, 2, 2, 1, 3, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 2, 1, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 2, 0, 1, 1, 0, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 2, 2, 2, 1, 0, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 1, 2, 0, 1, 3, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 2, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 2, 2, 1, 2, 1, 0, 0, 0, 1, 0, 3, 2, 2, 2, 0, 1, 1, 1, 0, 1, 3, 0, 0, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 2, 0, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 3, 0, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 1, 0, 0, 2, 2, 1, 1, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 0, 2, 0, 2, 1, 2, 2, 2, 0, 2, 1, 0, 2, 1, 0, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 0, 0, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 0, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 1, 1, 3, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 0, 2, 0, 1, 0, 2, 1, 2, 0, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 0, 1, 1, 2, 2, 2, 2, 2, 2, 0, 3, 0, 0, 0, 3, 2, 2, 1, 1, 0, 3, 2, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 3, 0, 1, 2, 2, 0, 1, 1, 2, 1, 2, 0, 0, 0, 0, 3, 0, 3, 0, 1, 0, 0, 2, 0, 3, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 1, 0, 0, 0, 2, 2, 0, 1, 1, 1, 2, 2, 1, 1, 1, 0, 2, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 0, 1, 0, 3, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 0, 0, 1, 1, 0, 2, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 3, 2, 1, 3, 2, 2, 1, 1, 1, 1, 0, 2, 1, 0, 2, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 2, 2, 2, 1, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 0, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 0, 0, 2, 2, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 0, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 0, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# for i in labeled_dataset:\n",
    "#     print(i['labels'])\n",
    "#     break\n",
    "\n",
    "label_int = [data['labels'].item() for data in labeled_dataset]\n",
    "print(label_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e4ac48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00210526 0.00022774 0.00068871 0.0006215 ]\n",
      "tensor([0.0006, 0.0002, 0.0002,  ..., 0.0006, 0.0006, 0.0006],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "count = Counter(label_int)\n",
    "class_count = np.array([count.get(idx) for idx in range(len(encoding))])\n",
    "weight = 1./class_count\n",
    "print(weight)\n",
    "samples_weight = np.array([weight[t] for t in label_int])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "print(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b15dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Build model \n",
    "model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                      num_labels=config.class_num, \n",
    "                                                      output_attentions=False, \n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "# Criterion & optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8) #or AdamW\n",
    "# total_steps = len(labeled_dataset) * epochs\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=0,\n",
    "#                                             num_training_steps=total_steps\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b287acde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial train module\n",
      "train_epoch 0\n",
      "Training Loss per 1000 steps: 1.4241783618927002\n",
      "Training Accuracy per 1000 steps: 6.25\n",
      "Training Loss Epoch: 0.6912925983028065\n",
      "Training Accuracy Epoch: 74.32824523779487\n",
      "Validation Loss per 1000 steps: 4.138816833496094\n",
      "Validation Accuracy per 1000 steps: 40.625\n",
      "Validation Loss Epoch: 5.1617597382395495\n",
      "Validation Accuracy Epoch: 20.28397565922921\n",
      "Test Loss per 1000 steps: 4.212571144104004\n",
      "Test Accuracy per 1000 steps: 40.625\n",
      "Test Loss Epoch: 5.718214758804867\n",
      "Test Accuracy Epoch: 10.97424412094065\n",
      "train_epoch 1\n",
      "Training Loss per 1000 steps: 4.918692588806152\n",
      "Training Accuracy per 1000 steps: 28.125\n",
      "Training Loss Epoch: 0.7245903590453728\n",
      "Training Accuracy Epoch: 75.21130314116311\n",
      "Validation Loss per 1000 steps: 3.3611509799957275\n",
      "Validation Accuracy per 1000 steps: 40.625\n",
      "Validation Loss Epoch: 4.138680005986845\n",
      "Validation Accuracy Epoch: 24.746450304259636\n",
      "Validation loss decreased (inf --> 4.138680).  \n",
      "train_epoch 2\n",
      "Training Loss per 1000 steps: 4.02781343460083\n",
      "Training Accuracy per 1000 steps: 28.125\n",
      "Training Loss Epoch: 0.6480933138650031\n",
      "Training Accuracy Epoch: 76.83865270594171\n",
      "Validation Loss per 1000 steps: 4.4631266593933105\n",
      "Validation Accuracy per 1000 steps: 40.625\n",
      "Validation Loss Epoch: 5.88097755917378\n",
      "Validation Accuracy Epoch: 21.8052738336714\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    }
   ],
   "source": [
    "# Init Trainer\n",
    "save_path = './test/'\n",
    "trainer = Trainer(config, model, optimizer, save_path, dev_dataset, test_dataset)\n",
    "\n",
    "# Initial training (supervised leraning)\n",
    "trainer.initial_train(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228adfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint \n",
    "checkpoint_path = trainer.sup_path +'/checkpoint.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "del model, optimizer, trainer.model, trainer.optimizer\n",
    "model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                      num_labels=config.class_num, \n",
    "                                                      output_attentions=False, \n",
    "                                                      output_hidden_states=False).to(config.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "trainer.model = model\n",
    "trainer.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5522228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fcdb184c220>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2364d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluator import Evaluator\n",
    "\n",
    "# # eval supervised trained model \n",
    "# Evaluator.evaluate(model, test_loader, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cafb95ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psudo-label 0/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 10:34:40.115733: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 10:34:40.231263: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-12 10:34:40.687308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-12 10:34:40.687353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-12 10:34:40.687357: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled 7927 unlabeled 10000\n",
      "pseudo-labeled 0\n",
      "After updated -> labeled 7927 unlabeled 10000\n",
      "outer_epoch 0 inner_epoch 0\n",
      "train_epoch 0\n",
      "Training Loss per 1000 steps: 4.018381595611572\n",
      "Training Accuracy per 1000 steps: 28.125\n",
      "Training Loss Epoch: 0.6426773996152464\n",
      "Training Accuracy Epoch: 76.69988646398386\n",
      "Validation Loss per 1000 steps: 3.9169087409973145\n",
      "Validation Accuracy per 1000 steps: 40.625\n",
      "Validation Loss Epoch: 4.256488610539706\n",
      "Validation Accuracy Epoch: 28.093306288032455\n",
      "Test Loss per 1000 steps: 4.029982566833496\n",
      "Test Accuracy per 1000 steps: 37.5\n",
      "Test Loss Epoch: 4.670109042099544\n",
      "Test Accuracy Epoch: 21.16461366181411\n",
      "outer_epoch 0 inner_epoch 1\n",
      "train_epoch 1\n",
      "Training Loss per 1000 steps: 4.780878067016602\n",
      "Training Accuracy per 1000 steps: 28.125\n",
      "Training Loss Epoch: 0.5081275421712969\n",
      "Training Accuracy Epoch: 83.5498927715403\n",
      "Validation Loss per 1000 steps: 3.441494941711426\n",
      "Validation Accuracy per 1000 steps: 43.75\n",
      "Validation Loss Epoch: 4.096371051044233\n",
      "Validation Accuracy Epoch: 28.70182555780933\n",
      "Validation loss decreased (inf --> 4.096371).  \n",
      "outer_epoch 0 inner_epoch 2\n",
      "train_epoch 2\n",
      "Training Loss per 1000 steps: 4.620060920715332\n",
      "Training Accuracy per 1000 steps: 28.125\n",
      "Training Loss Epoch: 0.40422348763827504\n",
      "Training Accuracy Epoch: 87.88949161094992\n",
      "Validation Loss per 1000 steps: 4.271477699279785\n",
      "Validation Accuracy per 1000 steps: 40.625\n",
      "Validation Loss Epoch: 4.750814024681946\n",
      "Validation Accuracy Epoch: 26.67342799188641\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Test Loss per 1000 steps: 3.934988498687744\n",
      "Test Accuracy per 1000 steps: 43.75\n",
      "Test Loss Epoch: 5.225640454462597\n",
      "Test Accuracy Epoch: 19.372900335946248\n",
      "psudo-label 6/24\n",
      "labeled 7927 unlabeled 10000\n",
      "pseudo-labeled 24\n",
      "After updated -> labeled 7951 unlabeled 9976\n",
      "outer_epoch 1 inner_epoch 0\n",
      "train_epoch 0\n",
      "Training Loss per 1000 steps: 4.8614912033081055\n",
      "Training Accuracy per 1000 steps: 28.125\n",
      "Training Loss Epoch: 0.3621893403383092\n",
      "Training Accuracy Epoch: 90.31568356181613\n",
      "Validation Loss per 1000 steps: 4.003913879394531\n",
      "Validation Accuracy per 1000 steps: 40.625\n",
      "Validation Loss Epoch: 4.123457566865029\n",
      "Validation Accuracy Epoch: 33.164300202839755\n",
      "Test Loss per 1000 steps: 3.7328178882598877\n",
      "Test Accuracy per 1000 steps: 43.75\n",
      "Test Loss Epoch: 4.548777922987938\n",
      "Test Accuracy Epoch: 26.651735722284435\n",
      "outer_epoch 1 inner_epoch 1\n",
      "train_epoch 1\n",
      "Training Loss per 1000 steps: 4.834992408752441\n",
      "Training Accuracy per 1000 steps: 31.25\n",
      "Training Loss Epoch: 0.27571887991028315\n",
      "Training Accuracy Epoch: 93.05747704691234\n",
      "Validation Loss per 1000 steps: 3.582916498184204\n",
      "Validation Accuracy per 1000 steps: 43.75\n",
      "Validation Loss Epoch: 4.060184406657373\n",
      "Validation Accuracy Epoch: 36.40973630831643\n",
      "Validation loss decreased (inf --> 4.060184).  \n",
      "outer_epoch 1 inner_epoch 2\n",
      "train_epoch 2\n",
      "Training Loss per 1000 steps: 3.857858657836914\n",
      "Training Accuracy per 1000 steps: 43.75\n",
      "Training Loss Epoch: 0.2204533647287667\n",
      "Training Accuracy Epoch: 94.70506854483713\n",
      "Validation Loss per 1000 steps: 3.925105333328247\n",
      "Validation Accuracy per 1000 steps: 46.875\n",
      "Validation Loss Epoch: 4.165256892601328\n",
      "Validation Accuracy Epoch: 34.88843813387424\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Test Loss per 1000 steps: 3.5046780109405518\n",
      "Test Accuracy per 1000 steps: 46.875\n",
      "Test Loss Epoch: 4.718509001391275\n",
      "Test Accuracy Epoch: 27.32362821948488\n",
      "psudo-label 107/428\n",
      "labeled 7951 unlabeled 9976\n",
      "pseudo-labeled 428\n",
      "After updated -> labeled 8379 unlabeled 9548\n",
      "outer_epoch 2 inner_epoch 0\n",
      "train_epoch 0\n",
      "Training Loss per 1000 steps: 3.8597381114959717\n",
      "Training Accuracy per 1000 steps: 37.5\n",
      "Training Loss Epoch: 0.2547579818411432\n",
      "Training Accuracy Epoch: 93.80594343000358\n",
      "Validation Loss per 1000 steps: 2.172508716583252\n",
      "Validation Accuracy per 1000 steps: 56.25\n",
      "Validation Loss Epoch: 2.5387759689361817\n",
      "Validation Accuracy Epoch: 49.49290060851927\n",
      "Test Loss per 1000 steps: 1.9731074571609497\n",
      "Test Accuracy per 1000 steps: 56.25\n",
      "Test Loss Epoch: 2.9432714815650667\n",
      "Test Accuracy Epoch: 44.90481522956327\n",
      "outer_epoch 2 inner_epoch 1\n",
      "train_epoch 1\n",
      "Training Loss per 1000 steps: 2.147671937942505\n",
      "Training Accuracy per 1000 steps: 62.5\n",
      "Training Loss Epoch: 0.17385325655816586\n",
      "Training Accuracy Epoch: 95.53646019811433\n",
      "Validation Loss per 1000 steps: 3.4139721393585205\n",
      "Validation Accuracy per 1000 steps: 46.875\n",
      "Validation Loss Epoch: 3.4230459447829955\n",
      "Validation Accuracy Epoch: 41.987829614604465\n",
      "EarlyStopping counter: 1 out of 3\n",
      "outer_epoch 2 inner_epoch 2\n",
      "train_epoch 2\n",
      "Training Loss per 1000 steps: 2.487589120864868\n",
      "Training Accuracy per 1000 steps: 59.375\n",
      "Training Loss Epoch: 0.13223566982929794\n",
      "Training Accuracy Epoch: 97.01635039980904\n",
      "Validation Loss per 1000 steps: 4.0835280418396\n",
      "Validation Accuracy per 1000 steps: 40.625\n",
      "Validation Loss Epoch: 3.995509224553262\n",
      "Validation Accuracy Epoch: 41.27789046653144\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Test Loss per 1000 steps: 3.7149736881256104\n",
      "Test Accuracy per 1000 steps: 46.875\n",
      "Test Loss Epoch: 4.473008790186474\n",
      "Test Accuracy Epoch: 35.61030235162374\n",
      "Best accuracy 44.90481522956327\n"
     ]
    }
   ],
   "source": [
    "# self-training\n",
    "trainer.self_train(labeled_dataset, unlabeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07650ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# eval semi-supervised trained model \n",
    "checkpoint_path = trainer.ssl_path +'/checkpoint.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "del model, optimizer, trainer.model, trainer.optimizer\n",
    "model = BertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                      num_labels=config.class_num, \n",
    "                                                      output_attentions=False, \n",
    "                                                      output_hidden_states=False).to(config.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "trainer.model = model\n",
    "trainer.optimizer = optimizer\n",
    "\n",
    "# trainer.evaluator.evaluate(trainer.model, trainer.test_loader, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "197cde86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        ids = batch['input_ids'].to(config.device, dtype=torch.long)\n",
    "        attention_mask = batch['attention_mask'].to(config.device, dtype=torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(config.device, dtype=torch.long)\n",
    "        targets = batch['labels'].to(config.device, dtype=torch.long)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0354eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, **config.test_params)\n",
    "probs = bert_predict(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d1e9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# fileObj = open(f'../results/self-training_finetuning_scibert_original_costsensitive_probs_fold_{fold}.obj', 'wb')\n",
    "# pickle.dump(probs,fileObj)\n",
    "# fileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07998ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# preds = np.where(probs[:, 1] > 0.5, 1, 0)\n",
    "preds = np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f3162e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.90482%\n",
      "Precision Score: 52.24900\n",
      "Recall Score: 59.00267\n",
      "F1 Score: 42.52735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     MENTION      0.266     0.791     0.398        67\n",
      "     NOTALGO      0.867     0.380     0.528       550\n",
      "         USE      0.725     0.281     0.405       178\n",
      "      EXTEND      0.232     0.908     0.369        98\n",
      "\n",
      "    accuracy                          0.449       893\n",
      "   macro avg      0.522     0.590     0.425       893\n",
      "weighted avg      0.724     0.449     0.477       893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "usage_class_names = ['MENTION', 'NOTALGO', 'USE', 'EXTEND']\n",
    "utilize_class_names = ['NOTUTILIZE', 'UTILIZE']\n",
    "\n",
    "y_test_encoded = test_labels\n",
    "accuracy = accuracy_score(y_test_encoded, preds)\n",
    "precision = precision_score(y_test_encoded, preds, average='macro')\n",
    "recall = recall_score(y_test_encoded, preds, average='macro')\n",
    "f1 = f1_score(y_test_encoded, preds, average='macro')\n",
    "\n",
    "print(\"Accuracy: %.5f%%\" % (accuracy*100))\n",
    "print(\"Precision Score: %.5f\" % (precision*100))\n",
    "print(\"Recall Score: %.5f\" % (recall*100))\n",
    "print(\"F1 Score: %.5f\" % (f1*100))\n",
    "\n",
    "print(classification_report(y_test_encoded, preds, target_names=usage_class_names, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bf7fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acccf3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# evaluate_roc(probs, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af30203",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_encoding = {v: k for k, v in encoding.items()}\n",
    "pred_label = [inv_encoding[key] for key in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbed6c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextID</th>\n",
       "      <th>citationID</th>\n",
       "      <th>CITATIONS_CONTEXTS</th>\n",
       "      <th>USAGE_LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202719</td>\n",
       "      <td>213441</td>\n",
       "      <td>ploitation of dependencies among facts. We pla...</td>\n",
       "      <td>MENTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>429629</td>\n",
       "      <td>460179</td>\n",
       "      <td>ic context free language [24], and all such la...</td>\n",
       "      <td>MENTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507299</td>\n",
       "      <td>541772</td>\n",
       "      <td>ch that routing delay is minimized. 5 Implemen...</td>\n",
       "      <td>USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>777941</td>\n",
       "      <td>830816</td>\n",
       "      <td>ch defeat applicable link layer compression me...</td>\n",
       "      <td>MENTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>834166</td>\n",
       "      <td>891644</td>\n",
       "      <td>ative probability of non-default 5 curve. In o...</td>\n",
       "      <td>USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>106711021</td>\n",
       "      <td>98946060</td>\n",
       "      <td>ommitment is distinguished from jobssatisfacti...</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>147628218</td>\n",
       "      <td>129938559</td>\n",
       "      <td>or countries thatshttp://www.fin.ee/index.php?...</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>43035149</td>\n",
       "      <td>41462937</td>\n",
       "      <td>x an integer r ? 2 and let A = e ?i 2r . The q...</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>101745998</td>\n",
       "      <td>95273024</td>\n",
       "      <td>ate the transcriptional effects of liganded NR...</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>202686562</td>\n",
       "      <td>176330510</td>\n",
       "      <td>rate proteins on rRNA gene promoters. The asse...</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     contextID  citationID                                 CITATIONS_CONTEXTS  \\\n",
       "0       202719      213441  ploitation of dependencies among facts. We pla...   \n",
       "1       429629      460179  ic context free language [24], and all such la...   \n",
       "2       507299      541772  ch that routing delay is minimized. 5 Implemen...   \n",
       "3       777941      830816  ch defeat applicable link layer compression me...   \n",
       "4       834166      891644  ative probability of non-default 5 curve. In o...   \n",
       "..         ...         ...                                                ...   \n",
       "888  106711021    98946060  ommitment is distinguished from jobssatisfacti...   \n",
       "889  147628218   129938559  or countries thatshttp://www.fin.ee/index.php?...   \n",
       "890   43035149    41462937  x an integer r ? 2 and let A = e ?i 2r . The q...   \n",
       "891  101745998    95273024  ate the transcriptional effects of liganded NR...   \n",
       "892  202686562   176330510  rate proteins on rRNA gene promoters. The asse...   \n",
       "\n",
       "    USAGE_LABELS  \n",
       "0        MENTION  \n",
       "1        MENTION  \n",
       "2            USE  \n",
       "3        MENTION  \n",
       "4            USE  \n",
       "..           ...  \n",
       "888      NOTALGO  \n",
       "889      NOTALGO  \n",
       "890      NOTALGO  \n",
       "891      NOTALGO  \n",
       "892      NOTALGO  \n",
       "\n",
       "[893 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_context = df_test.iloc[:, :3].copy()\n",
    "df_test_labels = df_test[['USAGE_LABELS']].copy()\n",
    "df_test_new = pd.concat([df_test_context, df_test_labels], axis=1)\n",
    "df_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "050df686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_new['PRED_LABELS'] = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0e5e9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextID</th>\n",
       "      <th>citationID</th>\n",
       "      <th>CITATIONS_CONTEXTS</th>\n",
       "      <th>USAGE_LABELS</th>\n",
       "      <th>PRED_LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202719</td>\n",
       "      <td>213441</td>\n",
       "      <td>ploitation of dependencies among facts. We pla...</td>\n",
       "      <td>MENTION</td>\n",
       "      <td>USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>429629</td>\n",
       "      <td>460179</td>\n",
       "      <td>ic context free language [24], and all such la...</td>\n",
       "      <td>MENTION</td>\n",
       "      <td>MENTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507299</td>\n",
       "      <td>541772</td>\n",
       "      <td>ch that routing delay is minimized. 5 Implemen...</td>\n",
       "      <td>USE</td>\n",
       "      <td>USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>777941</td>\n",
       "      <td>830816</td>\n",
       "      <td>ch defeat applicable link layer compression me...</td>\n",
       "      <td>MENTION</td>\n",
       "      <td>USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>834166</td>\n",
       "      <td>891644</td>\n",
       "      <td>ative probability of non-default 5 curve. In o...</td>\n",
       "      <td>USE</td>\n",
       "      <td>USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>106711021</td>\n",
       "      <td>98946060</td>\n",
       "      <td>ommitment is distinguished from jobssatisfacti...</td>\n",
       "      <td>NOTALGO</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>147628218</td>\n",
       "      <td>129938559</td>\n",
       "      <td>or countries thatshttp://www.fin.ee/index.php?...</td>\n",
       "      <td>NOTALGO</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>43035149</td>\n",
       "      <td>41462937</td>\n",
       "      <td>x an integer r ? 2 and let A = e ?i 2r . The q...</td>\n",
       "      <td>NOTALGO</td>\n",
       "      <td>USE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>101745998</td>\n",
       "      <td>95273024</td>\n",
       "      <td>ate the transcriptional effects of liganded NR...</td>\n",
       "      <td>NOTALGO</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>202686562</td>\n",
       "      <td>176330510</td>\n",
       "      <td>rate proteins on rRNA gene promoters. The asse...</td>\n",
       "      <td>NOTALGO</td>\n",
       "      <td>NOTALGO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     contextID  citationID                                 CITATIONS_CONTEXTS  \\\n",
       "0       202719      213441  ploitation of dependencies among facts. We pla...   \n",
       "1       429629      460179  ic context free language [24], and all such la...   \n",
       "2       507299      541772  ch that routing delay is minimized. 5 Implemen...   \n",
       "3       777941      830816  ch defeat applicable link layer compression me...   \n",
       "4       834166      891644  ative probability of non-default 5 curve. In o...   \n",
       "..         ...         ...                                                ...   \n",
       "888  106711021    98946060  ommitment is distinguished from jobssatisfacti...   \n",
       "889  147628218   129938559  or countries thatshttp://www.fin.ee/index.php?...   \n",
       "890   43035149    41462937  x an integer r ? 2 and let A = e ?i 2r . The q...   \n",
       "891  101745998    95273024  ate the transcriptional effects of liganded NR...   \n",
       "892  202686562   176330510  rate proteins on rRNA gene promoters. The asse...   \n",
       "\n",
       "    USAGE_LABELS PRED_LABELS  \n",
       "0        MENTION         USE  \n",
       "1        MENTION     MENTION  \n",
       "2            USE         USE  \n",
       "3        MENTION         USE  \n",
       "4            USE         USE  \n",
       "..           ...         ...  \n",
       "888      NOTALGO     NOTALGO  \n",
       "889      NOTALGO     NOTALGO  \n",
       "890      NOTALGO         USE  \n",
       "891      NOTALGO     NOTALGO  \n",
       "892      NOTALGO     NOTALGO  \n",
       "\n",
       "[893 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8919fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_new.to_csv(f'../Self-Training_Fine-Tuning_SciBERT_USAGE_new_Fold_{fold}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f6714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbbaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59635da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = {label:[] for label in range(2)}\n",
    "# print(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9d7fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pseudo_labeling(unlabeled_dataset, confidence_threshold):\n",
    "#     unlabeled_loader = DataLoader(unlabeled_dataset, **config.unlabeled_params)\n",
    "#     model.eval()\n",
    "#     new_dataset = {label:[] for label in range(2)}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for _, batch in enumerate(unlabeled_loader):\n",
    "#             ids = batch['input_ids'].to(config.device, dtype=torch.long)\n",
    "#             attention_mask = batch['attention_mask'].to(config.device, dtype=torch.long)\n",
    "#             token_type_ids = batch['token_type_ids'].to(config.device, dtype=torch.long)\n",
    "#             targets = batch['labels'].to(config.device, dtype=torch.long)\n",
    "\n",
    "#             outputs = model(ids, attention_mask, token_type_ids, labels=targets)\n",
    "#             loss, logits = outputs[0], outputs[1]\n",
    "#             confidences = torch.softmax(logits, dim=-1)\n",
    "#             big_val, big_idx = torch.max(confidences.data, dim=-1)\n",
    "\n",
    "#             for text_id, label, conf_val, target in zip(ids, big_idx, big_val, targets):\n",
    "#                 pred_label, conf_val, target = label.item(), conf_val.item(), target.item()\n",
    "#                 if conf_val >= confidence_threshold:\n",
    "#                     new_dataset[pred_label].append((text_id, pred_label, target))\n",
    "\n",
    "#     num_of_min_dataset = 987654321\n",
    "#     for label, dataset in new_dataset.items():\n",
    "#         if num_of_min_dataset > len(dataset):\n",
    "#             num_of_min_dataset = len(dataset)\n",
    "\n",
    "#     for label in new_dataset.keys():\n",
    "#         new_dataset[label] = new_dataset[label][:num_of_min_dataset]\n",
    "\n",
    "#     total, correct = 0, 0\n",
    "#     balanced_dataset = []\n",
    "#     for label in new_dataset.keys():\n",
    "#         balanced_dataset.extend(new_dataset[label][:num_of_min_dataset])\n",
    "\n",
    "#     for data in balanced_dataset:\n",
    "#         text_id, pred_label, target = data[0], data[1], data[2]\n",
    "#         if pred_label == target:\n",
    "#             correct+=1\n",
    "#         total+=1\n",
    "\n",
    "#     print('pseduo-label {}/{}'.format(correct, total))\n",
    "#     return balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdb06dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = pseudo_labeling(unlabeled_dataset, confidence_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bddec570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextID</th>\n",
       "      <th>citationID</th>\n",
       "      <th>CITATIONS_CONTEXTS</th>\n",
       "      <th>UTILIZE_LABELS</th>\n",
       "      <th>CUE_WORDS</th>\n",
       "      <th>CUE_HUMAN</th>\n",
       "      <th>CUE_SUBJECT</th>\n",
       "      <th>CUE_QUANTITY</th>\n",
       "      <th>CUE_FREQUENCY</th>\n",
       "      <th>CUE_TENSE</th>\n",
       "      <th>...</th>\n",
       "      <th>CUE_ALGOCLASS</th>\n",
       "      <th>CUE_USE</th>\n",
       "      <th>CUE_EXTEND</th>\n",
       "      <th>CUE_PROPOSE</th>\n",
       "      <th>CUE_EXPLAIN</th>\n",
       "      <th>CUE_ALGOKEY</th>\n",
       "      <th>CUE_DOCELKEY</th>\n",
       "      <th>CUE_PREP</th>\n",
       "      <th>USAGE_LABELS</th>\n",
       "      <th>CUE_WORDS_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73793</td>\n",
       "      <td>73896</td>\n",
       "      <td>ic is a function of, we need to monitor its si...</td>\n",
       "      <td>UTILIZE</td>\n",
       "      <td>fig. of procedure in describe algo pseudocode ...</td>\n",
       "      <td>we</td>\n",
       "      <td>algorithm we</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>describe</td>\n",
       "      <td>describe shown</td>\n",
       "      <td>pseudocode algorithm procedure</td>\n",
       "      <td>pseudocode algorithm procedure fig.</td>\n",
       "      <td>in of</td>\n",
       "      <td>USE</td>\n",
       "      <td>shown describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81011</td>\n",
       "      <td>81256</td>\n",
       "      <td>ty restraints were included for bases that wer...</td>\n",
       "      <td>NOTUTILIZE</td>\n",
       "      <td>for at of procedure extend us in structure alg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>calculation</td>\n",
       "      <td>using</td>\n",
       "      <td>extended</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm procedure</td>\n",
       "      <td>algorithm procedure</td>\n",
       "      <td>for in of</td>\n",
       "      <td>MENTION</td>\n",
       "      <td>extended using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124172</td>\n",
       "      <td>125234</td>\n",
       "      <td>ontradictory. Figure 19 shows an error that ar...</td>\n",
       "      <td>NOTUTILIZE</td>\n",
       "      <td>figure show in presented algo into routine pre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>figure algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>presented</td>\n",
       "      <td>presented shows</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>figure algorithm</td>\n",
       "      <td>on in into</td>\n",
       "      <td>MENTION</td>\n",
       "      <td>shows presented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152509</td>\n",
       "      <td>156833</td>\n",
       "      <td>each algorithm. Future work is to formalize an...</td>\n",
       "      <td>NOTUTILIZE</td>\n",
       "      <td>for of in us based on on following use develop...</td>\n",
       "      <td>we</td>\n",
       "      <td>algorithm we</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>on of</td>\n",
       "      <td>MENTION</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>165264</td>\n",
       "      <td>171552</td>\n",
       "      <td>of batch processing. When the actual time seri...</td>\n",
       "      <td>NOTUTILIZE</td>\n",
       "      <td>for proposed of measure in us use algo between...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proposed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>between for of when in</td>\n",
       "      <td>MENTION</td>\n",
       "      <td>proposed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>201652323</td>\n",
       "      <td>239174775</td>\n",
       "      <td>algorithms/parameters which is then followed b...</td>\n",
       "      <td>UTILIZE</td>\n",
       "      <td>of scheme use our differ techniques algo we sc...</td>\n",
       "      <td>we our</td>\n",
       "      <td>paper we our</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>analyses</td>\n",
       "      <td>use</td>\n",
       "      <td>NaN</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>for in of</td>\n",
       "      <td>USE</td>\n",
       "      <td>use present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7923</th>\n",
       "      <td>201650768</td>\n",
       "      <td>239172222</td>\n",
       "      <td>f components, the communication scheme for the...</td>\n",
       "      <td>UTILIZE</td>\n",
       "      <td>for of in propose algo system proposes scheme ...</td>\n",
       "      <td>we</td>\n",
       "      <td>algorithm we</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proposes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>for in of</td>\n",
       "      <td>USE</td>\n",
       "      <td>proposes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>201647360</td>\n",
       "      <td>239167273</td>\n",
       "      <td>al nonconvex problem (14) is solvable. It is e...</td>\n",
       "      <td>UTILIZE</td>\n",
       "      <td>at in problem develop algo method on us using ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>problem</td>\n",
       "      <td>using</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USE</td>\n",
       "      <td>using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7925</th>\n",
       "      <td>201633780</td>\n",
       "      <td>239146636</td>\n",
       "      <td>ed and used the Gonzalez-Perez et al. (2012) b...</td>\n",
       "      <td>UTILIZE</td>\n",
       "      <td>for performance of in us on use our algo compa...</td>\n",
       "      <td>we our</td>\n",
       "      <td>algorithm we our</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>algorithm</td>\n",
       "      <td>for of</td>\n",
       "      <td>USE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>201633779</td>\n",
       "      <td>239146636</td>\n",
       "      <td>iotti and Altman (2011) mutation datasets, we ...</td>\n",
       "      <td>UTILIZE</td>\n",
       "      <td>for performance of in on our algo can method a...</td>\n",
       "      <td>we our</td>\n",
       "      <td>we our</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>for of</td>\n",
       "      <td>USE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7927 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      contextID  citationID  \\\n",
       "0         73793       73896   \n",
       "1         81011       81256   \n",
       "2        124172      125234   \n",
       "3        152509      156833   \n",
       "4        165264      171552   \n",
       "...         ...         ...   \n",
       "7922  201652323   239174775   \n",
       "7923  201650768   239172222   \n",
       "7924  201647360   239167273   \n",
       "7925  201633780   239146636   \n",
       "7926  201633779   239146636   \n",
       "\n",
       "                                     CITATIONS_CONTEXTS UTILIZE_LABELS  \\\n",
       "0     ic is a function of, we need to monitor its si...        UTILIZE   \n",
       "1     ty restraints were included for bases that wer...     NOTUTILIZE   \n",
       "2     ontradictory. Figure 19 shows an error that ar...     NOTUTILIZE   \n",
       "3     each algorithm. Future work is to formalize an...     NOTUTILIZE   \n",
       "4     of batch processing. When the actual time seri...     NOTUTILIZE   \n",
       "...                                                 ...            ...   \n",
       "7922  algorithms/parameters which is then followed b...        UTILIZE   \n",
       "7923  f components, the communication scheme for the...        UTILIZE   \n",
       "7924  al nonconvex problem (14) is solvable. It is e...        UTILIZE   \n",
       "7925  ed and used the Gonzalez-Perez et al. (2012) b...        UTILIZE   \n",
       "7926  iotti and Altman (2011) mutation datasets, we ...        UTILIZE   \n",
       "\n",
       "                                              CUE_WORDS CUE_HUMAN  \\\n",
       "0     fig. of procedure in describe algo pseudocode ...        we   \n",
       "1     for at of procedure extend us in structure alg...       NaN   \n",
       "2     figure show in presented algo into routine pre...       NaN   \n",
       "3     for of in us based on on following use develop...        we   \n",
       "4     for proposed of measure in us use algo between...       NaN   \n",
       "...                                                 ...       ...   \n",
       "7922  of scheme use our differ techniques algo we sc...    we our   \n",
       "7923  for of in propose algo system proposes scheme ...        we   \n",
       "7924  at in problem develop algo method on us using ...       NaN   \n",
       "7925  for performance of in us on use our algo compa...    we our   \n",
       "7926  for performance of in on our algo can method a...    we our   \n",
       "\n",
       "           CUE_SUBJECT CUE_QUANTITY CUE_FREQUENCY CUE_TENSE  ...  \\\n",
       "0         algorithm we          NaN           NaN       NaN  ...   \n",
       "1            algorithm          NaN           NaN       NaN  ...   \n",
       "2     figure algorithm          NaN           NaN       NaN  ...   \n",
       "3         algorithm we          NaN           NaN       NaN  ...   \n",
       "4            algorithm          NaN           NaN       NaN  ...   \n",
       "...                ...          ...           ...       ...  ...   \n",
       "7922      paper we our          NaN           NaN       NaN  ...   \n",
       "7923      algorithm we          NaN           NaN       NaN  ...   \n",
       "7924         algorithm          NaN           NaN       NaN  ...   \n",
       "7925  algorithm we our          NaN           NaN       NaN  ...   \n",
       "7926            we our          NaN           NaN       NaN  ...   \n",
       "\n",
       "     CUE_ALGOCLASS CUE_USE CUE_EXTEND CUE_PROPOSE      CUE_EXPLAIN  \\\n",
       "0              NaN     NaN        NaN    describe   describe shown   \n",
       "1      calculation   using   extended         NaN              NaN   \n",
       "2              NaN     NaN        NaN   presented  presented shows   \n",
       "3              NaN     NaN        NaN         NaN              NaN   \n",
       "4              NaN     NaN        NaN    proposed              NaN   \n",
       "...            ...     ...        ...         ...              ...   \n",
       "7922      analyses     use        NaN     present          present   \n",
       "7923           NaN     NaN        NaN    proposes              NaN   \n",
       "7924       problem   using        NaN         NaN              NaN   \n",
       "7925           NaN     NaN        NaN         NaN              NaN   \n",
       "7926           NaN     NaN        NaN         NaN              NaN   \n",
       "\n",
       "                         CUE_ALGOKEY                         CUE_DOCELKEY  \\\n",
       "0     pseudocode algorithm procedure  pseudocode algorithm procedure fig.   \n",
       "1                algorithm procedure                  algorithm procedure   \n",
       "2                          algorithm                     figure algorithm   \n",
       "3                          algorithm                            algorithm   \n",
       "4                          algorithm                            algorithm   \n",
       "...                              ...                                  ...   \n",
       "7922                             NaN                                  NaN   \n",
       "7923                       algorithm                            algorithm   \n",
       "7924                       algorithm                            algorithm   \n",
       "7925                       algorithm                            algorithm   \n",
       "7926                             NaN                                  NaN   \n",
       "\n",
       "                    CUE_PREP USAGE_LABELS      CUE_WORDS_2  \n",
       "0                      in of          USE   shown describe  \n",
       "1                  for in of      MENTION   extended using  \n",
       "2                 on in into      MENTION  shows presented  \n",
       "3                      on of      MENTION              NaN  \n",
       "4     between for of when in      MENTION         proposed  \n",
       "...                      ...          ...              ...  \n",
       "7922               for in of          USE      use present  \n",
       "7923               for in of          USE         proposes  \n",
       "7924                     NaN          USE            using  \n",
       "7925                  for of          USE              NaN  \n",
       "7926                  for of          USE              NaN  \n",
       "\n",
       "[7927 rows x 29 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e914b081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextID</th>\n",
       "      <th>citationID</th>\n",
       "      <th>CONTENTS</th>\n",
       "      <th>LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81092137</td>\n",
       "      <td>83882792</td>\n",
       "      <td>regret minimization algorithms. .1.1 RELATED ...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29076091</td>\n",
       "      <td>28364380</td>\n",
       "      <td>f the mean curvature flow, for which one subtr...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5255081</td>\n",
       "      <td>5080973</td>\n",
       "      <td>density 3. In [7, 35], the authors proved lin...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40181031</td>\n",
       "      <td>41787608</td>\n",
       "      <td>terationsandavoidinghalosofunusedspacearound m...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87451901</td>\n",
       "      <td>91615254</td>\n",
       "      <td>by Buchin et al. [8]. Despite extensive resea...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>97580257</td>\n",
       "      <td>104838051</td>\n",
       "      <td>ise variance, resulting in better weight estim...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>82013109</td>\n",
       "      <td>84967007</td>\n",
       "      <td>nalysis software is limited to multi-body syst...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>196206171</td>\n",
       "      <td>231257845</td>\n",
       "      <td>r that cansbe used to identify uniquely the re...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>191997292</td>\n",
       "      <td>225056136</td>\n",
       "      <td>the Genetic Algorithm[2],sParticle Swarm Algor...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>48839085</td>\n",
       "      <td>48992858</td>\n",
       "      <td>al number of 102 candidates were identified in...</td>\n",
       "      <td>UNK_UNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      contextID citationID                                           CONTENTS  \\\n",
       "0      81092137   83882792   regret minimization algorithms. .1.1 RELATED ...   \n",
       "1      29076091   28364380  f the mean curvature flow, for which one subtr...   \n",
       "2       5255081    5080973   density 3. In [7, 35], the authors proved lin...   \n",
       "3      40181031   41787608  terationsandavoidinghalosofunusedspacearound m...   \n",
       "4      87451901   91615254   by Buchin et al. [8]. Despite extensive resea...   \n",
       "...         ...        ...                                                ...   \n",
       "9995   97580257  104838051  ise variance, resulting in better weight estim...   \n",
       "9996   82013109   84967007  nalysis software is limited to multi-body syst...   \n",
       "9997  196206171  231257845  r that cansbe used to identify uniquely the re...   \n",
       "9998  191997292  225056136  the Genetic Algorithm[2],sParticle Swarm Algor...   \n",
       "9999   48839085   48992858  al number of 102 candidates were identified in...   \n",
       "\n",
       "       LABELS  \n",
       "0     UNK_UNK  \n",
       "1     UNK_UNK  \n",
       "2     UNK_UNK  \n",
       "3     UNK_UNK  \n",
       "4     UNK_UNK  \n",
       "...       ...  \n",
       "9995  UNK_UNK  \n",
       "9996  UNK_UNK  \n",
       "9997  UNK_UNK  \n",
       "9998  UNK_UNK  \n",
       "9999  UNK_UNK  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6199ec75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextID</th>\n",
       "      <th>citationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202719</td>\n",
       "      <td>213441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>429629</td>\n",
       "      <td>460179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507299</td>\n",
       "      <td>541772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>777941</td>\n",
       "      <td>830816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>834166</td>\n",
       "      <td>891644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>97580257</td>\n",
       "      <td>104838051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>82013109</td>\n",
       "      <td>84967007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>196206171</td>\n",
       "      <td>231257845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>191997292</td>\n",
       "      <td>225056136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>48839085</td>\n",
       "      <td>48992858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10893 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      contextID citationID\n",
       "0        202719     213441\n",
       "1        429629     460179\n",
       "2        507299     541772\n",
       "3        777941     830816\n",
       "4        834166     891644\n",
       "...         ...        ...\n",
       "9995   97580257  104838051\n",
       "9996   82013109   84967007\n",
       "9997  196206171  231257845\n",
       "9998  191997292  225056136\n",
       "9999   48839085   48992858\n",
       "\n",
       "[10893 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check = pd.concat([df_test[['contextID', 'citationID']].copy(), df_unlabeled[['contextID', 'citationID']].copy()], axis=0)\n",
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca894f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextID</th>\n",
       "      <th>citationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202719</td>\n",
       "      <td>213441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>429629</td>\n",
       "      <td>460179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507299</td>\n",
       "      <td>541772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>777941</td>\n",
       "      <td>830816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>834166</td>\n",
       "      <td>891644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>97580257</td>\n",
       "      <td>104838051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>82013109</td>\n",
       "      <td>84967007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>196206171</td>\n",
       "      <td>231257845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>191997292</td>\n",
       "      <td>225056136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>48839085</td>\n",
       "      <td>48992858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10889 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      contextID citationID\n",
       "0        202719     213441\n",
       "1        429629     460179\n",
       "2        507299     541772\n",
       "3        777941     830816\n",
       "4        834166     891644\n",
       "...         ...        ...\n",
       "9995   97580257  104838051\n",
       "9996   82013109   84967007\n",
       "9997  196206171  231257845\n",
       "9998  191997292  225056136\n",
       "9999   48839085   48992858\n",
       "\n",
       "[10889 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc50ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
